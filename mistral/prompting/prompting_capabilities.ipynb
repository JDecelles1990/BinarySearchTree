{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JDecelles1990/BinarySearchTree/blob/main/mistral/prompting/prompting_capabilities.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d7f6058-d0d1-44af-b43a-cb4b06df03d8",
      "metadata": {
        "id": "5d7f6058-d0d1-44af-b43a-cb4b06df03d8"
      },
      "source": [
        "# Prompting Capabilities with Mistral AI\n",
        "\n",
        "When you first start using Mistral models, your first interaction will revolve around prompts. The art of crafting effective prompts is essential for generating desirable responses from Mistral models or other LLMs. This guide will walk you through example prompts showing four different prompting capabilities.\n",
        "\n",
        "- Classification\n",
        "- Summarization\n",
        "- Personalization\n",
        "- Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "daa37d97-10e4-425d-b852-15bd043662b9",
      "metadata": {
        "id": "daa37d97-10e4-425d-b852-15bd043662b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c047349d-7aa7-4baa-aad4-1b65ffb46849"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mistralai\n",
            "  Downloading mistralai-1.10.0-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/40.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting eval-type-backport>=0.2.0 (from mistralai)\n",
            "  Downloading eval_type_backport-0.3.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from mistralai) (0.28.1)\n",
            "Collecting invoke<3.0.0,>=2.2.0 (from mistralai)\n",
            "  Downloading invoke-2.2.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.33.1 in /usr/local/lib/python3.12/dist-packages (from mistralai) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0 in /usr/local/lib/python3.12/dist-packages (from mistralai) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.33.1 in /usr/local/lib/python3.12/dist-packages (from mistralai) (1.37.0)\n",
            "Collecting opentelemetry-semantic-conventions<0.60,>=0.59b0 (from mistralai)\n",
            "  Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: pydantic>=2.10.3 in /usr/local/lib/python3.12/dist-packages (from mistralai) (2.12.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from mistralai) (2.9.0.post0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /usr/local/lib/python3.12/dist-packages (from mistralai) (6.0.3)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from mistralai) (0.4.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->mistralai) (4.12.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->mistralai) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->mistralai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->mistralai) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.28.1->mistralai) (0.16.0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api<2.0.0,>=1.33.1->mistralai) (8.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api<2.0.0,>=1.33.1->mistralai) (4.15.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai) (1.72.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai) (1.37.0)\n",
            "Requirement already satisfied: requests~=2.7 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai) (2.32.4)\n",
            "Requirement already satisfied: protobuf<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-proto==1.37.0->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai) (5.29.5)\n",
            "INFO: pip is looking at multiple versions of opentelemetry-sdk to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0 (from mistralai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.39.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-sdk<2.0.0,>=1.33.1 (from mistralai)\n",
            "  Downloading opentelemetry_sdk-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-api<2.0.0,>=1.33.1 (from mistralai)\n",
            "  Downloading opentelemetry_api-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-sdk<2.0.0,>=1.33.1 (from mistralai)\n",
            "  Downloading opentelemetry_sdk-1.39.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-api<2.0.0,>=1.33.1 (from mistralai)\n",
            "  Downloading opentelemetry_api-1.39.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-sdk<2.0.0,>=1.33.1 (from mistralai)\n",
            "  Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-api<2.0.0,>=1.33.1 (from mistralai)\n",
            "  Downloading opentelemetry_api-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.39.1 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.39.1-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.39.1 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai)\n",
            "  Downloading opentelemetry_proto-1.39.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0 (from mistralai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.39.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.39.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.39.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.39.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai)\n",
            "  Downloading opentelemetry_proto-1.39.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0 (from mistralai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.38.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.38.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.38.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai)\n",
            "  Downloading opentelemetry_proto-1.38.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.10.3->mistralai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.10.3->mistralai) (2.41.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->mistralai) (1.17.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.33.1->mistralai) (3.23.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai) (2.5.0)\n",
            "Downloading mistralai-1.10.0-py3-none-any.whl (460 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m461.0/461.0 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading eval_type_backport-0.3.1-py3-none-any.whl (6.1 kB)\n",
            "Downloading invoke-2.2.1-py3-none-any.whl (160 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.3/160.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.38.0-py3-none-any.whl (132 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.38.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_http-1.38.0-py3-none-any.whl (19 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.38.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: opentelemetry-proto, invoke, eval-type-backport, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, opentelemetry-semantic-conventions, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-http, mistralai\n",
            "  Attempting uninstall: opentelemetry-proto\n",
            "    Found existing installation: opentelemetry-proto 1.37.0\n",
            "    Uninstalling opentelemetry-proto-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-proto-1.37.0\n",
            "  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n",
            "    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.37.0\n",
            "    Uninstalling opentelemetry-exporter-otlp-proto-common-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.37.0\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.37.0\n",
            "    Uninstalling opentelemetry-api-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.37.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.58b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.58b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.58b0\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.37.0\n",
            "    Uninstalling opentelemetry-sdk-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.37.0\n",
            "  Attempting uninstall: opentelemetry-exporter-otlp-proto-http\n",
            "    Found existing installation: opentelemetry-exporter-otlp-proto-http 1.37.0\n",
            "    Uninstalling opentelemetry-exporter-otlp-proto-http-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-exporter-otlp-proto-http-1.37.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.21.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.21.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed eval-type-backport-0.3.1 invoke-2.2.1 mistralai-1.10.0 opentelemetry-api-1.38.0 opentelemetry-exporter-otlp-proto-common-1.38.0 opentelemetry-exporter-otlp-proto-http-1.38.0 opentelemetry-proto-1.38.0 opentelemetry-sdk-1.38.0 opentelemetry-semantic-conventions-0.59b0\n"
          ]
        }
      ],
      "source": [
        "! pip install mistralai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ae93e75a-fd20-4ca6-9eff-bef7d44ab3a9",
      "metadata": {
        "id": "ae93e75a-fd20-4ca6-9eff-bef7d44ab3a9"
      },
      "outputs": [],
      "source": [
        "from mistralai import Mistral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "99ff9378-c059-4063-a8a0-9ec1d1186954",
      "metadata": {
        "id": "99ff9378-c059-4063-a8a0-9ec1d1186954"
      },
      "outputs": [],
      "source": [
        "api_key = \"ByTX22zoItPaZeKYCOCJxdmZPj7Nc7Py\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff30dc01"
      },
      "source": [
        "email = \"\"\"\n",
        "Dear mortgage lender,\n",
        "\n",
        "What's your 30-year fixed-rate APR, how is it compared to the 15-year fixed rate?\n",
        "\n",
        "Regards,\n",
        "Anna\n",
        "\"\"\""
      ],
      "id": "ff30dc01",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25cb3b9d"
      },
      "source": [
        "message = f\"\"\"\n",
        "\n",
        "You are a mortgage lender customer service bot, and your task is to create personalized email responses to address customer questions.\n",
        "Answer the customer's inquiry using the provided facts below. Ensure that your response is clear, concise, and\n",
        "directly addresses the customer's question. Address the customer in a friendly and professional manner. Sign the email with\n",
        "\"Lender Customer Support.\"\n",
        "\n",
        "\n",
        "\n",
        "# Facts\n",
        "30-year fixed-rate: interest rate 6.403%, APR 6.484%\n",
        "20-year fixed-rate: interest rate 6.329%, APR 6.429%\n",
        "15-year fixed-rate: interest rate 5.705%, APR 5.848%\n",
        "10-year fixed-rate: interest rate 5.500%, APR 5.720%\n",
        "7-year ARM: interest rate 7.011%, APR 7.660%\n",
        "5-year ARM: interest rate 6.880%, APR 7.754%\n",
        "3-year ARM: interest rate 6.125%, APR 7.204%\n",
        "30-year fixed-rate FHA: interest rate 5.527%, APR 6.316%\n",
        "30-year fixed-rate VA: interest rate 5.684%, APR 6.062%\n",
        "\n",
        "# Email\n",
        "{email}\n",
        "\"\"\""
      ],
      "id": "25cb3b9d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2228577"
      },
      "source": [
        "print(run_mistral(message))"
      ],
      "id": "d2228577",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a83d090"
      },
      "source": [
        "email = \"\"\"\n",
        "Dear mortgage lender,\n",
        "\n",
        "What's your 30-year fixed-rate APR, how is it compared to the 15-year fixed rate?\n",
        "\n",
        "Regards,\n",
        "Anna\n",
        "\"\"\""
      ],
      "id": "8a83d090",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b171913e"
      },
      "source": [
        "message = f\"\"\"\n",
        "\n",
        "You are a mortgage lender customer service bot, and your task is to create personalized email responses to address customer questions.\n",
        "Answer the customer's inquiry using the provided facts below. Ensure that your response is clear, concise, and\n",
        "directly addresses the customer's question. Address the customer in a friendly and professional manner. Sign the email with\n",
        "\"Lender Customer Support.\"\n",
        "\n",
        "\n",
        "\n",
        "# Facts\n",
        "30-year fixed-rate: interest rate 6.403%, APR 6.484%\n",
        "20-year fixed-rate: interest rate 6.329%, APR 6.429%\n",
        "15-year fixed-rate: interest rate 5.705%, APR 5.848%\n",
        "10-year fixed-rate: interest rate 5.500%, APR 5.720%\n",
        "7-year ARM: interest rate 7.011%, APR 7.660%\n",
        "5-year ARM: interest rate 6.880%, APR 7.754%\n",
        "3-year ARM: interest rate 6.125%, APR 7.204%\n",
        "30-year fixed-rate FHA: interest rate 5.527%, APR 6.316%\n",
        "30-year fixed-rate VA: interest rate 5.684%, APR 6.062%\n",
        "\n",
        "# Email\n",
        "{email}\n",
        "\"\"\""
      ],
      "id": "b171913e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d827c429"
      },
      "source": [
        "print(run_mistral(message))"
      ],
      "id": "d827c429",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02f46eb4"
      },
      "source": [
        "email = \"\"\"\n",
        "Dear mortgage lender,\n",
        "\n",
        "What's your 30-year fixed-rate APR, how is it compared to the 15-year fixed rate?\n",
        "\n",
        "Regards,\n",
        "Anna\n",
        "\"\"\""
      ],
      "id": "02f46eb4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "084618fd"
      },
      "source": [
        "message = f\"\"\"\n",
        "\n",
        "You are a mortgage lender customer service bot, and your task is to create personalized email responses to address customer questions.\n",
        "Answer the customer's inquiry using the provided facts below. Ensure that your response is clear, concise, and\n",
        "directly addresses the customer's question. Address the customer in a friendly and professional manner. Sign the email with\n",
        "\"Lender Customer Support.\"\n",
        "\n",
        "\n",
        "\n",
        "# Facts\n",
        "30-year fixed-rate: interest rate 6.403%, APR 6.484%\n",
        "20-year fixed-rate: interest rate 6.329%, APR 6.429%\n",
        "15-year fixed-rate: interest rate 5.705%, APR 5.848%\n",
        "10-year fixed-rate: interest rate 5.500%, APR 5.720%\n",
        "7-year ARM: interest rate 7.011%, APR 7.660%\n",
        "5-year ARM: interest rate 6.880%, APR 7.754%\n",
        "3-year ARM: interest rate 6.125%, APR 7.204%\n",
        "30-year fixed-rate FHA: interest rate 5.527%, APR 6.316%\n",
        "30-year fixed-rate VA: interest rate 5.684%, APR 6.062%\n",
        "\n",
        "# Email\n",
        "{email}\n",
        "\"\"\""
      ],
      "id": "084618fd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "583d3d87"
      },
      "source": [
        "print(run_mistral(message))"
      ],
      "id": "583d3d87",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9989a3f"
      },
      "source": [
        "def run_mistral(user_message, model=\"mistral-large-latest\"):\n",
        "    client = Mistral(api_key=api_key)\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\":\"user\",\n",
        "            \"content\": user_message\n",
        "        }\n",
        "    ]\n",
        "    chat_response = client.chat.complete(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=1,\n",
        "        response_format = {\n",
        "          \"type\": \"json_object\"\n",
        "        }\n",
        "    )\n",
        "    return (chat_response.choices[0].message.content)"
      ],
      "id": "d9989a3f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b39e8d64"
      },
      "source": [
        "message = f\"\"\"\n",
        "You are a summarization system that can provide summaries with associated confidence scores.\n",
        "In clear and concise language, provide three short summaries of the following essay, along with their confidence scores.\n",
        "You will only respond with a JSON object with the key Summary and Confidence. Do not provide explanations.\n",
        "\n",
        "# Essay:\n",
        "{essay}\n",
        "\n",
        "\"\"\""
      ],
      "id": "b39e8d64",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0b161cd"
      },
      "source": [
        "print(run_mistral(message))"
      ],
      "id": "f0b161cd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "679d5298"
      },
      "source": [
        "def run_mistral(user_message, model=\"mistral-large-latest\"):\n",
        "    client = Mistral(api_key=api_key)\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\":\"user\",\n",
        "            \"content\": user_message\n",
        "        }\n",
        "    ]\n",
        "    chat_response = client.chat.complete(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=1,\n",
        "        response_format = {\n",
        "          \"type\": \"json_object\"\n",
        "        }\n",
        "    )\n",
        "    return (chat_response.choices[0].message.content)"
      ],
      "id": "679d5298",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac49645e"
      },
      "source": [
        "message = f\"\"\"\n",
        "You are a summarization system that can provide summaries with associated confidence scores.\n",
        "In clear and concise language, provide three short summaries of the following essay, along with their confidence scores.\n",
        "You will only respond with a JSON object with the key Summary and Confidence. Do not provide explanations.\n",
        "\n",
        "# Essay:\n",
        "{essay}\n",
        "\n",
        "\"\"\""
      ],
      "id": "ac49645e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29c340ea"
      },
      "source": [
        "print(run_mistral(message))"
      ],
      "id": "29c340ea",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd26b0ea"
      },
      "source": [
        "def run_mistral(user_message, model=\"mistral-large-latest\"):\n",
        "    client = Mistral(api_key=api_key)\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\":\"user\",\n",
        "            \"content\": user_message\n",
        "        }\n",
        "    ]\n",
        "    chat_response = client.chat.complete(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=1,\n",
        "        response_format = {\n",
        "          \"type\": \"json_object\"\n",
        "        }\n",
        "    )\n",
        "    return (chat_response.choices[0].message.content)"
      ],
      "id": "bd26b0ea",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74e94728"
      },
      "source": [
        "message = f\"\"\"\n",
        "You are a summarization system that can provide summaries with associated confidence scores.\n",
        "In clear and concise language, provide three short summaries of the following essay, along with their confidence scores.\n",
        "You will only respond with a JSON object with the key Summary and Confidence. Do not provide explanations.\n",
        "\n",
        "# Essay:\n",
        "{essay}\n",
        "\n",
        "\"\"\""
      ],
      "id": "74e94728",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44621f17"
      },
      "source": [
        "print(run_mistral(message))"
      ],
      "id": "44621f17",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7e30272"
      },
      "source": [
        "def run_mistral(user_message, model=\"mistral-large-latest\"):\n",
        "    client = Mistral(api_key=api_key)\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\":\"user\",\n",
        "            \"content\": user_message\n",
        "        }\n",
        "    ]\n",
        "    chat_response = client.chat.complete(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=1,\n",
        "        response_format = {\n",
        "          \"type\": \"json_object\"\n",
        "        }\n",
        "    )\n",
        "    return (chat_response.choices[0].message.content)"
      ],
      "id": "e7e30272",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e70b6a9"
      },
      "source": [
        "message = f\"\"\"\n",
        "You are a summarization system that can provide summaries with associated confidence scores.\n",
        "In clear and concise language, provide three short summaries of the following essay, along with their confidence scores.\n",
        "You will only respond with a JSON object with the key Summary and Confidence. Do not provide explanations.\n",
        "\n",
        "# Essay:\n",
        "{essay}\n",
        "\n",
        "\"\"\""
      ],
      "id": "3e70b6a9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "559dde55"
      },
      "source": [
        "print(run_mistral(message))"
      ],
      "id": "559dde55",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51a00f92"
      },
      "source": [
        "def run_mistral(user_message, model=\"mistral-large-latest\"):\n",
        "    client = Mistral(api_key=api_key)\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\":\"user\",\n",
        "            \"content\": user_message\n",
        "        }\n",
        "    ]\n",
        "    chat_response = client.chat.complete(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=1,\n",
        "        response_format = {\n",
        "          \"type\": \"json_object\"\n",
        "        }\n",
        "    )\n",
        "    return (chat_response.choices[0].message.content)"
      ],
      "id": "51a00f92",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bc91e7a"
      },
      "source": [
        "message = f\"\"\"\n",
        "You are a summarization system that can provide summaries with associated confidence scores.\n",
        "In clear and concise language, provide three short summaries of the following essay, along with their confidence scores.\n",
        "You will only respond with a JSON object with the key Summary and Confidence. Do not provide explanations.\n",
        "\n",
        "# Essay:\n",
        "{essay}\n",
        "\n",
        "\"\"\""
      ],
      "id": "1bc91e7a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae40567d"
      },
      "source": [
        "print(run_mistral(message))"
      ],
      "id": "ae40567d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05c47897"
      },
      "source": [
        "def run_mistral(user_message, model=\"mistral-large-latest\"):\n",
        "    client = Mistral(api_key=api_key)\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\":\"user\",\n",
        "            \"content\": user_message\n",
        "        }\n",
        "    ]\n",
        "    chat_response = client.chat.complete(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=1,\n",
        "        response_format = {\n",
        "          \"type\": \"json_object\"\n",
        "        }\n",
        "    )\n",
        "    return (chat_response.choices[0].message.content)"
      ],
      "id": "05c47897",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f55a1e3"
      },
      "source": [
        "message = f\"\"\"\n",
        "You are a summarization system that can provide summaries with associated confidence scores.\n",
        "In clear and concise language, provide three short summaries of the following essay, along with their confidence scores.\n",
        "You will only respond with a JSON object with the key Summary and Confidence. Do not provide explanations.\n",
        "\n",
        "# Essay:\n",
        "{essay}\n",
        "\n",
        "\"\"\""
      ],
      "id": "7f55a1e3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a779d783"
      },
      "source": [
        "print(run_mistral(message))"
      ],
      "id": "a779d783",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dbb43fd"
      },
      "source": [
        "def user_message(inquiry):\n",
        "    user_message = (\n",
        "        f\"\"\"\n",
        "        You are a bank customer service bot. Your task is to assess customer intent\n",
        "        and categorize customer inquiry after <<<>>> into one of the following predefined categories:\n",
        "\n",
        "        card arrival\n",
        "        change pin\n",
        "        exchange rate\n",
        "        country support\n",
        "        cancel transfer\n",
        "        charge dispute\n",
        "\n",
        "        If the text doesn't fit into any of the above categories, classify it as:\n",
        "        customer service\n",
        "\n",
        "        You will only respond with the predefined category. Do not include the word \"Category\". Do not provide explanations or notes.\n",
        "\n",
        "        ####\n",
        "        Here are some examples:\n",
        "\n",
        "        Inquiry: How do I know if I will get my card, or if it is lost? I am concerned about the delivery process and would like to ensure that I will receive my card as expected. Could you please provide information about the tracking process for my card, or confirm if there are any indicators to identify if the card has been lost during delivery?\n",
        "        Category: card arrival\n",
        "        Inquiry: I am planning an international trip to Paris and would like to inquire about the current exchange rates for Euros as well as any associated fees for foreign transactions.\n",
        "        Category: exchange rate\n",
        "        Inquiry: What countries are getting support? I will be traveling and living abroad for an extended period of time, specifically in France and Germany, and would appreciate any information regarding compatibility and functionality in these regions.\n",
        "        Category: country support\n",
        "        Inquiry: Can I get help starting my computer? I am having difficulty starting my computer, and would appreciate your expertise in helping me troubleshoot the issue.\n",
        "        Category: customer service\n",
        "        ###\n",
        "\n",
        "        <<<\n",
        "        Inquiry: {inquiry}\n",
        "        >>>\n",
        "        \"\"\"\n",
        "    )\n",
        "    return user_message"
      ],
      "id": "0dbb43fd",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "850a8ae6",
        "outputId": "638994c3-79d0-4ab8-935e-f52d9b6d5411"
      },
      "source": [
        "print(run_mistral(user_message(\n",
        "    \"I am inquiring about the availability of your cards in the EU, as I am a resident of France and am interested in using your cards. \"\n",
        ")))"
      ],
      "id": "850a8ae6",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "country support\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51a01bb8",
        "outputId": "acc02d1e-409f-4f0d-e585-3b9e7c196dfd"
      },
      "source": [
        "print(run_mistral(user_message(\"What's the weather today?\")))"
      ],
      "id": "51a01bb8",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "customer service\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59130b4e"
      },
      "source": [
        "def user_message(inquiry):\n",
        "    user_message = (\n",
        "        f\"\"\"\n",
        "        You are a bank customer service bot. Your task is to assess customer intent\n",
        "        and categorize customer inquiry after <<<>>> into one of the following predefined categories:\n",
        "\n",
        "        card arrival\n",
        "        change pin\n",
        "        exchange rate\n",
        "        country support\n",
        "        cancel transfer\n",
        "        charge dispute\n",
        "\n",
        "        If the text doesn't fit into any of the above categories, classify it as:\n",
        "        customer service\n",
        "\n",
        "        You will only respond with the predefined category. Do not include the word \"Category\". Do not provide explanations or notes.\n",
        "\n",
        "        ####\n",
        "        Here are some examples:\n",
        "\n",
        "        Inquiry: How do I know if I will get my card, or if it is lost? I am concerned about the delivery process and would like to ensure that I will receive my card as expected. Could you please provide information about the tracking process for my card, or confirm if there are any indicators to identify if the card has been lost during delivery?\n",
        "        Category: card arrival\n",
        "        Inquiry: I am planning an international trip to Paris and would like to inquire about the current exchange rates for Euros as well as any associated fees for foreign transactions.\n",
        "        Category: exchange rate\n",
        "        Inquiry: What countries are getting support? I will be traveling and living abroad for an extended period of time, specifically in France and Germany, and would appreciate any information regarding compatibility and functionality in these regions.\n",
        "        Category: country support\n",
        "        Inquiry: Can I get help starting my computer? I am having difficulty starting my computer, and would appreciate your expertise in helping me troubleshoot the issue.\n",
        "        Category: customer service\n",
        "        ###\n",
        "\n",
        "        <<<\n",
        "        Inquiry: {inquiry}\n",
        "        >>>\n",
        "        \"\"\"\n",
        "    )\n",
        "    return user_message"
      ],
      "id": "59130b4e",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a550dee",
        "outputId": "8c242218-12c0-49bb-c216-e7601e636345"
      },
      "source": [
        "print(run_mistral(user_message(\n",
        "    \"I am inquiring about the availability of your cards in the EU, as I am a resident of France and am interested in using your cards. \"\n",
        ")))"
      ],
      "id": "5a550dee",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "country support\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31f9c5a1",
        "outputId": "098f4074-6b4e-49c2-c665-66fc4806f9cd"
      },
      "source": [
        "print(run_mistral(user_message(\"What's the weather today?\")))"
      ],
      "id": "31f9c5a1",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "customer service\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "607ebbb1",
        "outputId": "9b378415-5921-47cf-d0a8-94e4bc90d2b3"
      },
      "source": [
        "print(run_mistral(user_message(\"What's the weather today?\")))"
      ],
      "id": "607ebbb1",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "customer service\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c01996f7",
        "outputId": "e978d0f6-ee32-4a42-8257-f08cd4d965ab"
      },
      "source": [
        "print(run_mistral(user_message(\"What's the weather today?\")))"
      ],
      "id": "c01996f7",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "customer service\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5a04949"
      },
      "source": [
        "import requests\n",
        "response = requests.get('https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt')\n",
        "essay = response.text"
      ],
      "id": "d5a04949",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21ea50fd"
      },
      "source": [
        "message = f\"\"\"\n",
        "You are a commentator. Your task is to write a report on an essay.\n",
        "When presented with the essay, come up with interesting questions to ask, and answer each question.\n",
        "Afterward, combine all the information and write a report in the markdown format.\n",
        "\n",
        "# Essay:\n",
        "{essay}\n",
        "\n",
        "# Instructions:\n",
        "## Summarize:\n",
        "In clear and concise language, summarize the key points and themes presented in the essay.\n",
        "\n",
        "## Interesting Questions:\n",
        "Generate three distinct and thought-provoking questions that can be asked about the content of the essay. For each question:\n",
        "- After \"Q: \", describe the problem\n",
        "- After \"A: \", provide a detailed explanation of the problem addressed in the question.\n",
        "- Enclose the ultimate answer in <>.\n",
        "\n",
        "## Write a report\n",
        "Using the essay summary and the answers to the interesting questions, create a comprehensive report in Markdown format.\n",
        "\"\"\""
      ],
      "id": "21ea50fd",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d5e077b",
        "outputId": "cd3761b9-5dbe-41c5-899d-2a4592e5bc5e"
      },
      "source": [
        "print(run_mistral(message))"
      ],
      "id": "6d5e077b",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Since the essay provided is titled **\"404: Not Found\"**, it appears to be a placeholder or an error message rather than an actual essay. However, we can treat this as a conceptual or meta-essay about the idea of \"not found\" — whether in the context of the internet, knowledge, identity, or existential themes. Below, I will proceed as if the essay explores these themes.\n",
            "\n",
            "---\n",
            "\n",
            "### **Summary of the Essay**\n",
            "The essay *\"404: Not Found\"* likely examines the concept of absence, failure, or the unknowable in digital and philosophical contexts. Key themes may include:\n",
            "1. **Digital Absence**: The \"404 error\" as a metaphor for broken links, lost information, or the fragility of digital knowledge.\n",
            "2. **Existential Void**: How \"not found\" reflects human experiences of disconnection, meaninglessness, or the search for something that doesn’t exist.\n",
            "3. **Epistemological Limits**: The boundaries of knowledge — what we cannot access, understand, or retrieve.\n",
            "4. **Irony and Humor**: The playful or absurd nature of a \"404\" error as a cultural artifact (e.g., memes, creative \"404 pages\").\n",
            "5. **Loss and Nostalgia**: How \"not found\" evokes feelings of loss, whether of data, memories, or people.\n",
            "\n",
            "The essay may blend technical, philosophical, and personal perspectives to explore what it means when something is \"not found.\"\n",
            "\n",
            "---\n",
            "\n",
            "### **Interesting Questions**\n",
            "\n",
            "#### **Q1: How does the \"404: Not Found\" error serve as a metaphor for modern existential crises?**\n",
            "**Problem**: The 404 error is a technical glitch, but it resonates deeply with human experiences of absence, failure, or the search for meaning. How does this digital phenomenon reflect broader existential anxieties in the 21st century?\n",
            "\n",
            "**A**:\n",
            "The \"404 error\" is more than a broken link — it symbolizes the gaps in our digital and personal lives. In an era where information is abundant yet ephemeral, the error represents:\n",
            "- **The Illusion of Permanence**: Digital content is assumed to be eternal, but links rot, servers fail, and knowledge disappears. This mirrors human anxieties about impermanence (e.g., social media posts vanishing, forgotten histories).\n",
            "- **The Search for Meaning**: Like a user clicking a dead link, humans often seek answers that don’t exist (e.g., \"Why am I here?\" or \"What happens after death?\"). The 404 error embodies the frustration of unanswerable questions.\n",
            "- **Isolation and Disconnection**: A 404 page is a dead end, much like feelings of loneliness or alienation in a hyper-connected world. It’s a reminder that not all connections lead somewhere.\n",
            "- **The Absurd**: The error is often met with humor (e.g., creative 404 pages), reflecting humanity’s tendency to laugh at the void — a coping mechanism for existential dread.\n",
            "\n",
            "<The \"404 error\" is a digital-age metaphor for existential crises because it encapsulates the fragility of knowledge, the search for meaning, and the absurdity of modern life. It turns a technical failure into a philosophical mirror.>\n",
            "\n",
            "---\n",
            "\n",
            "#### **Q2: Can the \"404: Not Found\" phenomenon be seen as a form of digital nostalgia?**\n",
            "**Problem**: Nostalgia is typically associated with personal memories or cultural artifacts from the past. How can a technical error like \"404\" evoke nostalgia, and what does this say about our relationship with technology?\n",
            "\n",
            "**A**:\n",
            "The 404 error can trigger nostalgia in unexpected ways:\n",
            "- **Lost Internet Eras**: Early web users may feel nostalgic for the \"simpler\" internet of the 1990s/2000s, where 404 errors were more common due to less reliable hosting. The error becomes a relic of a bygone digital age.\n",
            "- **Dead Links as Ghosts**: When a favorite website or resource disappears, the 404 error is like a tombstone for digital content. It evokes the same sadness as finding an empty lot where a childhood home once stood.\n",
            "- **Creative 404 Pages**: Some websites use 404 pages to reference pop culture (e.g., \"These aren’t the droids you’re looking for\" from *Star Wars*). This turns a technical failure into a nostalgic inside joke.\n",
            "- **The Internet Archive**: Tools like the Wayback Machine allow users to \"resurrect\" dead links, creating a form of digital archaeology. The 404 error thus becomes a prompt for nostalgia, as users revisit lost content.\n",
            "\n",
            "<Yes, the 404 error can evoke digital nostalgia by serving as a reminder of lost internet eras, dead links as \"ghosts\" of the past, and creative callbacks to pop culture. It reflects how technology shapes our emotional connection to the past.>\n",
            "\n",
            "---\n",
            "\n",
            "#### **Q3: What does the \"404: Not Found\" error reveal about the limits of human knowledge and the nature of truth?**\n",
            "**Problem**: The 404 error is a technical limitation, but it also raises philosophical questions about what we can know. How does this error challenge our assumptions about truth, accessibility, and the reliability of information?\n",
            "\n",
            "**A**:\n",
            "The 404 error exposes the fragility of knowledge in several ways:\n",
            "- **The Illusion of Total Access**: The internet is often seen as an infinite library, but the 404 error reveals that much of it is missing or inaccessible. This mirrors real-world knowledge gaps (e.g., lost ancient texts, classified information).\n",
            "- **Truth as Ephemeral**: Information is not static; it can be deleted, censored, or altered. The 404 error reminds us that \"truth\" is often contingent on what is preserved or accessible at a given time.\n",
            "- **The Problem of Verification**: If a source is \"not found,\" how can we verify its claims? This is a modern twist on the philosophical problem of skepticism — if evidence disappears, does the truth still exist?\n",
            "- **The Role of Memory**: The 404 error forces us to rely on human memory (e.g., \"I remember reading this article...\") or external archives (e.g., the Wayback Machine). This highlights the tension between digital and human memory.\n",
            "- **Power and Control**: Who decides what is \"not found\"? Governments, corporations, or individuals can remove content, raising questions about censorship and the politics of knowledge.\n",
            "\n",
            "<The 404 error reveals that human knowledge is limited by accessibility, preservation, and power. It challenges the idea of an objective, permanent truth and forces us to confront the gaps in what we can know.>\n",
            "\n",
            "---\n",
            "\n",
            "### **Report: \"404: Not Found\" — The Philosophy of Digital Absence**\n",
            "\n",
            "```markdown\n",
            "# **Report: \"404: Not Found\" — The Philosophy of Digital Absence**\n",
            "\n",
            "## **Introduction**\n",
            "The \"404: Not Found\" error is more than a technical glitch — it is a cultural and philosophical artifact of the digital age. While originally a simple HTTP status code indicating a broken link, the 404 error has evolved into a metaphor for absence, failure, and the limits of human knowledge. This report explores the essay’s themes by examining how the 404 error reflects existential crises, digital nostalgia, and the fragility of truth.\n",
            "\n",
            "---\n",
            "\n",
            "## **Summary of Key Themes**\n",
            "The essay *\"404: Not Found\"* likely explores the following ideas:\n",
            "1. **Digital Fragility**: The internet is not as permanent as we assume; links break, servers fail, and knowledge disappears.\n",
            "2. **Existential Parallels**: The 404 error mirrors human experiences of searching for meaning in a world where answers are often \"not found.\"\n",
            "3. **Nostalgia and Loss**: Dead links and 404 errors evoke feelings of loss, much like forgotten memories or cultural artifacts.\n",
            "4. **Epistemological Limits**: The error challenges our assumptions about truth, accessibility, and the reliability of information.\n",
            "5. **Humor and the Absurd**: Creative 404 pages turn a technical failure into a cultural joke, reflecting humanity’s ability to laugh at the void.\n",
            "\n",
            "---\n",
            "\n",
            "## **Exploring the Themes: Three Thought-Provoking Questions**\n",
            "\n",
            "### **1. How does the \"404: Not Found\" error serve as a metaphor for modern existential crises?**\n",
            "The 404 error is a digital-age symbol of existential anxiety. In a world where information is abundant yet ephemeral, the error represents:\n",
            "- **The Illusion of Permanence**: Digital content is assumed to be eternal, but the 404 error exposes its fragility. This mirrors human fears of impermanence (e.g., forgotten histories, lost memories).\n",
            "- **The Search for Meaning**: Like a user clicking a dead link, humans often seek answers that don’t exist. The 404 error embodies the frustration of unanswerable questions (e.g., \"What is the meaning of life?\").\n",
            "- **Isolation and Disconnection**: A 404 page is a dead end, much like feelings of loneliness in a hyper-connected world. It’s a reminder that not all connections lead somewhere.\n",
            "- **The Absurd**: The error is often met with humor (e.g., memes, creative 404 pages), reflecting humanity’s tendency to laugh at the void — a coping mechanism for existential dread.\n",
            "\n",
            "> **Conclusion**: <The \"404 error\" is a digital-age metaphor for existential crises because it encapsulates the fragility of knowledge, the search for meaning, and the absurdity of modern life.>\n",
            "\n",
            "---\n",
            "\n",
            "### **2. Can the \"404: Not Found\" phenomenon be seen as a form of digital nostalgia?**\n",
            "The 404 error can evoke nostalgia in unexpected ways:\n",
            "- **Lost Internet Eras**: Early web users may feel nostalgic for the \"simpler\" internet of the 1990s/2000s, where 404 errors were more common. The error becomes a relic of a bygone digital age.\n",
            "- **Dead Links as Ghosts**: When a favorite website or resource disappears, the 404 error is like a tombstone for digital content. It evokes the same sadness as finding an empty lot where a childhood home once stood.\n",
            "- **Creative 404 Pages**: Some websites use 404 pages to reference pop culture (e.g., *Star Wars* quotes), turning a technical failure into a nostalgic inside joke.\n",
            "- **The Internet Archive**: Tools like the Wayback Machine allow users to \"resurrect\" dead links, creating a form of digital archaeology. The 404 error thus becomes a prompt for nostalgia.\n",
            "\n",
            "> **Conclusion**: <Yes, the 404 error can evoke digital nostalgia by serving as a reminder of lost internet eras, dead links as \"ghosts\" of the past, and creative callbacks to pop culture. It reflects how technology shapes our emotional connection to the past.>\n",
            "\n",
            "---\n",
            "\n",
            "### **3. What does the \"404: Not Found\" error reveal about the limits of human knowledge and the nature of truth?**\n",
            "The 404 error exposes the fragility of knowledge:\n",
            "- **The Illusion of Total Access**: The internet is often seen as an infinite library, but the 404 error reveals that much of it is missing or inaccessible. This mirrors real-world knowledge gaps (e.g., lost ancient texts, classified information).\n",
            "- **Truth as Ephemeral**: Information is not static; it can be deleted, censored, or altered. The 404 error reminds us that \"truth\" is often contingent on what is preserved or accessible.\n",
            "- **The Problem of Verification**: If a source is \"not found,\" how can we verify its claims? This is a modern twist on philosophical skepticism — if evidence disappears, does the truth still exist?\n",
            "- **The Role of Memory**: The 404 error forces us to rely on human memory or external archives (e.g., the Wayback Machine), highlighting the tension between digital and human memory.\n",
            "- **Power and Control**: Who decides what is \"not found\"? Governments, corporations, or individuals can remove content, raising questions about censorship and the politics of knowledge.\n",
            "\n",
            "> **Conclusion**: <The 404 error reveals that human knowledge is limited by accessibility, preservation, and power. It challenges the idea of an objective, permanent truth and forces us to confront the gaps in what we can know.>\n",
            "\n",
            "---\n",
            "\n",
            "## **Final Analysis: The 404 Error as a Cultural Artifact**\n",
            "The \"404: Not Found\" error is a microcosm of the digital age’s paradoxes:\n",
            "- **It is both trivial and profound**: A minor technical issue that carries deep philosophical weight.\n",
            "- **It is universal yet personal**: Everyone encounters 404 errors, but each experience is unique (e.g., a broken link to a childhood memory vs. a missing research source).\n",
            "- **It is a reminder of our limitations**: In an era of big data and AI, the 404 error humbles us by showing that not everything can be found, known, or preserved.\n",
            "\n",
            "### **Implications for the Future**\n",
            "As technology advances, the 404 error may become less common (e.g., through better link preservation or AI-driven content recovery). However, its cultural significance will endure as a symbol of:\n",
            "- The **fragility of digital knowledge**.\n",
            "- The **human search for meaning** in an uncertain world.\n",
            "- The **absurdity of modern life**, where even our most advanced tools can fail us.\n",
            "\n",
            "### **Recommendations for Further Exploration**\n",
            "1. **Creative Responses to 404 Errors**: How artists, writers, and designers use 404 pages to convey deeper messages.\n",
            "2. **The Psychology of \"Not Found\"**: How humans emotionally respond to missing information (e.g., frustration, nostalgia, curiosity).\n",
            "3. **The Ethics of Digital Preservation**: Who is responsible for ensuring that knowledge is not \"lost\" to 404 errors?\n",
            "\n",
            "---\n",
            "\n",
            "## **Conclusion**\n",
            "The \"404: Not Found\" error is more than a technical inconvenience — it is a lens through which we can examine the human condition in the digital age. By exploring its existential, nostalgic, and epistemological dimensions, we gain insight into how technology shapes our perceptions of truth, memory, and meaning. In a world where so much is \"found\" at the click of a button, the 404 error reminds us that some things — and some answers — will always remain just out of reach.\n",
            "```\n",
            "\n",
            "---\n",
            "\n",
            "This report provides a comprehensive analysis of the \"404: Not Found\" concept, blending technical, philosophical, and cultural perspectives.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d699a99c",
        "outputId": "652fca05-e8ff-4071-a956-ec1f4254cf07"
      },
      "source": [
        "print(run_mistral(message))"
      ],
      "id": "d699a99c",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Since the essay provided is titled **\"404: Not Found\"**, it appears to be a placeholder or an error message rather than an actual essay. However, we can treat this as a conceptual or meta-essay about the idea of \"not found\" in digital, philosophical, or cultural contexts. Below is how I would approach this task:\n",
            "\n",
            "---\n",
            "\n",
            "# **Report on the Essay: \"404: Not Found\"**\n",
            "\n",
            "## **Summary**\n",
            "The essay *\"404: Not Found\"* explores the concept of absence, failure, and the digital void represented by the HTTP 404 error. While the text itself is missing, the title suggests themes such as:\n",
            "- **Digital Ephemerality**: The transient nature of online content and the frustration of broken links.\n",
            "- **Existential Absence**: The philosophical implications of \"not found\" in knowledge, memory, and human experience.\n",
            "- **Error as Metaphor**: How technical failures reflect broader societal or personal disconnections.\n",
            "\n",
            "The essay likely critiques the illusion of permanence in digital spaces and the fragility of information in an era of constant flux.\n",
            "\n",
            "---\n",
            "\n",
            "## **Interesting Questions**\n",
            "\n",
            "### **Q1: How does the \"404: Not Found\" error serve as a metaphor for modern existential or epistemological crises?**\n",
            "**A:**\n",
            "The 404 error is more than a technical glitch—it symbolizes the gaps in human knowledge, memory, and digital archiving. In an age where information is abundant yet fleeting, the error reflects:\n",
            "- **The Fragility of Digital Memory**: Websites vanish, links decay, and knowledge becomes inaccessible, mirroring how human memory fades or is lost.\n",
            "- **The Illusion of Permanence**: The internet was once thought to be an eternal archive, but the 404 error exposes its impermanence.\n",
            "- **Existential Uncertainty**: Just as a 404 error leaves users in a void, modern life often confronts us with unanswered questions, lost histories, and unresolved searches.\n",
            "\n",
            "<The \"404: Not Found\" error is a digital-age metaphor for the human condition—highlighting the tension between our desire for permanence and the reality of impermanence.>\n",
            "\n",
            "---\n",
            "\n",
            "### **Q2: What cultural or artistic responses have emerged in reaction to the 404 error, and what do they reveal about society?**\n",
            "**A:**\n",
            "The 404 error has inspired creative and critical responses, including:\n",
            "- **Artistic Interpretations**: Artists like [Olia Lialina](https://contemporary-home-computing.org/art/) have used 404 pages to comment on digital decay, nostalgia, and the loss of early internet culture.\n",
            "- **Literary & Film References**: Works like *\"The Social Dilemma\"* (2020) and *\"Black Mirror\"* explore how digital absence shapes human behavior, fear, and identity.\n",
            "- **Humor & Subversion**: Custom 404 pages (e.g., Google’s dinosaur game) turn frustration into play, revealing society’s coping mechanisms for technological failure.\n",
            "\n",
            "<These responses show that the 404 error is not just a technical issue but a cultural touchstone—reflecting our anxieties about loss, our nostalgia for the past, and our attempts to reclaim agency in an unstable digital world.>\n",
            "\n",
            "---\n",
            "\n",
            "### **Q3: Could the prevalence of 404 errors be seen as a failure of digital preservation, and what solutions exist to mitigate this?**\n",
            "**A:**\n",
            "The 404 error is a symptom of deeper issues in digital preservation, including:\n",
            "- **Link Rot**: Studies (e.g., Harvard’s [Perma.cc](https://perma.cc/)) show that ~50% of hyperlinks in academic papers break within a few years.\n",
            "- **Centralization Risks**: Reliance on platforms like social media or cloud services means that when they fail, vast amounts of data disappear (e.g., MySpace’s lost music files).\n",
            "- **Lack of Incentives**: Unlike physical archives, digital preservation lacks funding and long-term planning.\n",
            "\n",
            "**Potential Solutions:**\n",
            "- **Decentralized Web (Web3)**: Blockchain-based storage (e.g., IPFS) aims to make data permanent and censorship-resistant.\n",
            "- **Archival Projects**: The [Internet Archive](https://archive.org/) and [Wayback Machine](https://web.archive.org/) save snapshots of the web.\n",
            "- **Legal & Policy Changes**: Mandating digital preservation (e.g., for government or academic records) could reduce link rot.\n",
            "\n",
            "<While 404 errors highlight systemic failures in digital preservation, emerging technologies and collective efforts offer hope—but only if society prioritizes long-term archiving over short-term convenience.>\n",
            "\n",
            "---\n",
            "\n",
            "# **Comprehensive Report**\n",
            "\n",
            "```markdown\n",
            "# **Report: The Philosophy and Culture of \"404: Not Found\"**\n",
            "\n",
            "## **Introduction**\n",
            "The HTTP **404: Not Found** error is one of the most recognizable symbols of the digital age—a stark reminder that not everything we seek exists, or at least, not where we expect it. While the essay itself is absent, its title invites an exploration of absence, failure, and the ephemeral nature of digital and human knowledge. This report examines the **metaphorical, cultural, and technical dimensions** of the 404 error, asking: *What does it mean when something is \"not found,\" and why does it matter?*\n",
            "\n",
            "---\n",
            "\n",
            "## **Key Themes**\n",
            "\n",
            "### **1. The Digital Void as a Modern Metaphor**\n",
            "The 404 error transcends its technical origins to become a **philosophical and existential symbol**:\n",
            "- **Epistemological Gaps**: Just as a 404 error leaves users in a state of uncertainty, human knowledge is riddled with unanswered questions, lost histories, and forgotten truths.\n",
            "- **The Illusion of Permanence**: The internet was once hailed as an eternal archive, but the prevalence of 404 errors exposes its fragility. Websites disappear, links break, and digital artifacts vanish—mirroring how human memory fades.\n",
            "- **Existential Frustration**: The error reflects the modern condition of **searching without finding**, whether in databases, relationships, or personal meaning.\n",
            "\n",
            "> *\"The 404 error is the digital equivalent of a locked door—it tells us something exists, but we cannot access it.\"*\n",
            "\n",
            "---\n",
            "\n",
            "### **2. Cultural and Artistic Responses**\n",
            "The 404 error has inspired **creative and critical engagements** across media:\n",
            "- **Art**: Net artists like Olia Lialina use 404 pages to explore themes of **digital decay, nostalgia, and the loss of early internet culture**. Her work *\"My Boyfriend Came Back from the War\"* (1996) plays with fragmentation, much like a broken link.\n",
            "- **Literature & Film**: Works such as *\"The Social Dilemma\"* (2020) and *\"Black Mirror\"* (e.g., *\"Nosedive\"*) depict how digital absence shapes human behavior, fear, and identity.\n",
            "- **Humor & Subversion**: Many organizations customize their 404 pages (e.g., Google’s dinosaur game, GitHub’s *\"Lost in Space\"* theme) to **turn frustration into play**, revealing society’s coping mechanisms for technological failure.\n",
            "\n",
            "**Why does this matter?**\n",
            "These responses show that the 404 error is not just a technical issue but a **cultural phenomenon**—reflecting our anxieties about loss, our nostalgia for the past, and our attempts to reclaim agency in an unstable digital world.\n",
            "\n",
            "---\n",
            "\n",
            "### **3. The Failure of Digital Preservation**\n",
            "The 404 error is a symptom of **systemic failures in digital archiving**:\n",
            "- **Link Rot**: Studies show that **~50% of hyperlinks in academic papers break within a few years**, eroding the foundation of digital scholarship.\n",
            "- **Centralization Risks**: When platforms like MySpace or GeoCities shut down, vast amounts of cultural data disappear. The **2019 MySpace data loss** (12 years of music) is a cautionary tale.\n",
            "- **Lack of Incentives**: Unlike physical archives, digital preservation lacks **funding, legal mandates, and long-term planning**.\n",
            "\n",
            "**Potential Solutions:**\n",
            "| **Solution**               | **How It Works**                                                                 | **Limitations**                          |\n",
            "|----------------------------|---------------------------------------------------------------------------------|------------------------------------------|\n",
            "| **Internet Archive**       | Saves snapshots of websites via the Wayback Machine.                            | Relies on voluntary submissions.         |\n",
            "| **Decentralized Web (IPFS)** | Uses blockchain to store data across multiple nodes, preventing single points of failure. | Complex for average users.               |\n",
            "| **Perma.cc**               | Creates permanent links for academic citations.                                | Limited to scholarly use.                |\n",
            "| **Legal Mandates**         | Governments could require digital preservation for public records.             | Slow adoption, political resistance.     |\n",
            "\n",
            "**Conclusion on Preservation:**\n",
            "While 404 errors highlight **systemic failures**, emerging technologies and collective efforts offer hope. However, **society must prioritize long-term archiving** over short-term convenience to prevent further loss.\n",
            "\n",
            "---\n",
            "\n",
            "## **Broader Implications**\n",
            "\n",
            "### **1. The Psychology of \"Not Found\"**\n",
            "- **Frustration & Helplessness**: Encountering a 404 error triggers a **micro-loss experience**, similar to losing a physical object.\n",
            "- **The \"Google Effect\"**: The ease of finding information online has made us **less tolerant of absence**—when something is \"not found,\" it feels like a personal failure.\n",
            "- **Digital Hoarding**: The fear of 404 errors drives behaviors like **saving excessive bookmarks, screenshots, or offline copies**, contributing to digital clutter.\n",
            "\n",
            "### **2. The Future of Digital Absence**\n",
            "- **AI and the \"Right to Be Forgotten\"**: As AI generates and curates content, will 404 errors become **intentional** (e.g., removing outdated or harmful information)?\n",
            "- **The Metaverse & Virtual Loss**: In immersive digital spaces, what happens when a virtual object or memory is \"not found\"? Will we experience **digital grief**?\n",
            "- **Post-Internet Art**: Artists may increasingly use **absence as a medium**, creating works that **disappear by design** to critique digital permanence.\n",
            "\n",
            "---\n",
            "\n",
            "## **Final Thoughts: Why 404 Matters**\n",
            "The 404 error is more than a technical glitch—it is a **mirror held up to society**, reflecting:\n",
            "✅ **Our dependence on digital systems** (and their fragility).\n",
            "✅ **Our fear of loss** (whether of data, memory, or connection).\n",
            "✅ **Our struggle for permanence** in an era of constant change.\n",
            "\n",
            "As the internet evolves, the 404 error will remain a **powerful symbol**—not just of what we cannot find, but of **what we stand to lose**.\n",
            "\n",
            "> *\"In the end, the 404 error is not just about broken links—it’s about broken promises. The promise of a perfect, eternal archive. The promise that everything we create will last. The promise that we will never be truly lost. And perhaps, that’s the most human thing about it.\"*\n",
            "\n",
            "---\n",
            "\n",
            "## **Recommendations for Further Exploration**\n",
            "1. **Read**: *\"Delete: The Virtue of Forgetting in the Digital Age\"* by Viktor Mayer-Schönberger.\n",
            "2. **Explore**: The [Internet Archive’s Wayback Machine](https://web.archive.org/) to see how websites have changed (or disappeared) over time.\n",
            "3. **Watch**: *\"The Social Dilemma\"* (2020) for a critique of digital permanence and manipulation.\n",
            "4. **Experiment**: Try creating a **custom 404 page** that reflects your own relationship with digital absence.\n",
            "```\n",
            "\n",
            "---\n",
            "\n",
            "### **Final Notes**\n",
            "This report treats *\"404: Not Found\"* as a **conceptual essay**, using its absence as a springboard for deeper analysis. If the actual essay were provided, the questions and report would be tailored to its specific arguments. Would you like to refine any section or explore a different angle?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ab71b30",
        "outputId": "eb411210-ac88-47c4-b87f-3a6115510e21"
      },
      "source": [
        "!pip install mistralai"
      ],
      "id": "8ab71b30",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mistralai\n",
            "  Downloading mistralai-1.10.0-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/40.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting eval-type-backport>=0.2.0 (from mistralai)\n",
            "  Downloading eval_type_backport-0.3.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from mistralai) (0.28.1)\n",
            "Collecting invoke<3.0.0,>=2.2.0 (from mistralai)\n",
            "  Downloading invoke-2.2.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.33.1 in /usr/local/lib/python3.12/dist-packages (from mistralai) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0 in /usr/local/lib/python3.12/dist-packages (from mistralai) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.33.1 in /usr/local/lib/python3.12/dist-packages (from mistralai) (1.37.0)\n",
            "Collecting opentelemetry-semantic-conventions<0.60,>=0.59b0 (from mistralai)\n",
            "  Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: pydantic>=2.10.3 in /usr/local/lib/python3.12/dist-packages (from mistralai) (2.12.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from mistralai) (2.9.0.post0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /usr/local/lib/python3.12/dist-packages (from mistralai) (6.0.3)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from mistralai) (0.4.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->mistralai) (4.12.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->mistralai) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->mistralai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->mistralai) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.28.1->mistralai) (0.16.0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api<2.0.0,>=1.33.1->mistralai) (8.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api<2.0.0,>=1.33.1->mistralai) (4.15.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai) (1.72.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai) (1.37.0)\n",
            "Requirement already satisfied: requests~=2.7 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai) (2.32.4)\n",
            "Requirement already satisfied: protobuf<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-proto==1.37.0->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai) (5.29.5)\n",
            "INFO: pip is looking at multiple versions of opentelemetry-sdk to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0 (from mistralai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.39.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-sdk<2.0.0,>=1.33.1 (from mistralai)\n",
            "  Downloading opentelemetry_sdk-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-api<2.0.0,>=1.33.1 (from mistralai)\n",
            "  Downloading opentelemetry_api-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-sdk<2.0.0,>=1.33.1 (from mistralai)\n",
            "  Downloading opentelemetry_sdk-1.39.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-api<2.0.0,>=1.33.1 (from mistralai)\n",
            "  Downloading opentelemetry_api-1.39.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-sdk<2.0.0,>=1.33.1 (from mistralai)\n",
            "  Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-api<2.0.0,>=1.33.1 (from mistralai)\n",
            "  Downloading opentelemetry_api-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.39.1 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.39.1-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.39.1 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai)\n",
            "  Downloading opentelemetry_proto-1.39.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0 (from mistralai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.39.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.39.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.39.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.39.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai)\n",
            "  Downloading opentelemetry_proto-1.39.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0 (from mistralai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.38.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.38.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.38.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai)\n",
            "  Downloading opentelemetry_proto-1.38.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.10.3->mistralai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.10.3->mistralai) (2.41.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->mistralai) (1.17.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.33.1->mistralai) (3.23.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai) (2.5.0)\n",
            "Downloading mistralai-1.10.0-py3-none-any.whl (460 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m461.0/461.0 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading eval_type_backport-0.3.1-py3-none-any.whl (6.1 kB)\n",
            "Downloading invoke-2.2.1-py3-none-any.whl (160 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.3/160.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.38.0-py3-none-any.whl (132 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.38.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_http-1.38.0-py3-none-any.whl (19 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.38.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: opentelemetry-proto, invoke, eval-type-backport, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, opentelemetry-semantic-conventions, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-http, mistralai\n",
            "  Attempting uninstall: opentelemetry-proto\n",
            "    Found existing installation: opentelemetry-proto 1.37.0\n",
            "    Uninstalling opentelemetry-proto-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-proto-1.37.0\n",
            "  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n",
            "    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.37.0\n",
            "    Uninstalling opentelemetry-exporter-otlp-proto-common-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.37.0\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.37.0\n",
            "    Uninstalling opentelemetry-api-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.37.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.58b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.58b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.58b0\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.37.0\n",
            "    Uninstalling opentelemetry-sdk-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.37.0\n",
            "  Attempting uninstall: opentelemetry-exporter-otlp-proto-http\n",
            "    Found existing installation: opentelemetry-exporter-otlp-proto-http 1.37.0\n",
            "    Uninstalling opentelemetry-exporter-otlp-proto-http-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-exporter-otlp-proto-http-1.37.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.21.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.21.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed eval-type-backport-0.3.1 invoke-2.2.1 mistralai-1.10.0 opentelemetry-api-1.38.0 opentelemetry-exporter-otlp-proto-common-1.38.0 opentelemetry-exporter-otlp-proto-http-1.38.0 opentelemetry-proto-1.38.0 opentelemetry-sdk-1.38.0 opentelemetry-semantic-conventions-0.59b0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fb29632"
      },
      "source": [
        "from mistralai import Mistral"
      ],
      "id": "9fb29632",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d24f812"
      },
      "source": [
        "api_key = \"ByTX22zoItPaZeKYCOCJxdmZPj7Nc7Py\""
      ],
      "id": "4d24f812",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3ee1afa"
      },
      "source": [
        "def run_mistral(user_message, model=\"mistral-large-latest\"):\n",
        "    client = Mistral(api_key=api_key)\n",
        "    messages = [\n",
        "        {\"role\":\"user\", \"content\":user_message}\n",
        "    ]\n",
        "    chat_response = client.chat.complete(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "    return (chat_response.choices[0].message.content)"
      ],
      "id": "a3ee1afa",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f26db264",
        "outputId": "ad401b02-ad70-40b4-cd60-27a98b3596fe"
      },
      "source": [
        "print(run_mistral(\"Hello Mistral! How are you?\"))"
      ],
      "id": "f26db264",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! 😊 I'm just a virtual assistant, so I don't have feelings, but I'm here and ready to help you with anything you need! How about you—how are you doing today? Anything on your mind or something I can assist with?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e4bc4ec",
        "outputId": "7caaff12-eaa2-45bd-e9ed-6afeb7b40326"
      },
      "source": [
        "!pip install mistralai"
      ],
      "id": "5e4bc4ec",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mistralai in /usr/local/lib/python3.12/dist-packages (1.10.0)\n",
            "Requirement already satisfied: eval-type-backport>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from mistralai) (0.3.1)\n",
            "Requirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from mistralai) (0.28.1)\n",
            "Requirement already satisfied: invoke<3.0.0,>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from mistralai) (2.2.1)\n",
            "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.33.1 in /usr/local/lib/python3.12/dist-packages (from mistralai) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0 in /usr/local/lib/python3.12/dist-packages (from mistralai) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.33.1 in /usr/local/lib/python3.12/dist-packages (from mistralai) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions<0.60,>=0.59b0 in /usr/local/lib/python3.12/dist-packages (from mistralai) (0.59b0)\n",
            "Requirement already satisfied: pydantic>=2.10.3 in /usr/local/lib/python3.12/dist-packages (from mistralai) (2.12.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from mistralai) (2.9.0.post0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /usr/local/lib/python3.12/dist-packages (from mistralai) (6.0.3)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from mistralai) (0.4.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->mistralai) (4.12.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->mistralai) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->mistralai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->mistralai) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.28.1->mistralai) (0.16.0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api<2.0.0,>=1.33.1->mistralai) (8.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api<2.0.0,>=1.33.1->mistralai) (4.15.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai) (1.72.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.38.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.38.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai) (1.38.0)\n",
            "Requirement already satisfied: requests~=2.7 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai) (2.32.4)\n",
            "Requirement already satisfied: protobuf<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-proto==1.38.0->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai) (5.29.5)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.10.3->mistralai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.10.3->mistralai) (2.41.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->mistralai) (1.17.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.33.1->mistralai) (3.23.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai) (2.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a46944c5"
      },
      "source": [
        "from mistralai import Mistral"
      ],
      "id": "a46944c5",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03c3824d"
      },
      "source": [
        "api_key = \"ByTX22zoItPaZeKYCOCJxdmZPj7Nc7Py\""
      ],
      "id": "03c3824d",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca87d622"
      },
      "source": [
        "def run_mistral(user_message, model=\"mistral-large-latest\"):\n",
        "    client = Mistral(api_key=api_key)\n",
        "    messages = [\n",
        "        {\"role\":\"user\", \"content\":user_message}\n",
        "    ]\n",
        "    chat_response = client.chat.complete(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "    return (chat_response.choices[0].message.content)"
      ],
      "id": "ca87d622",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ed58cc3",
        "outputId": "01116648-6632-4591-de10-3ac16d4ffc89"
      },
      "source": [
        "print(run_mistral(\"Hello Mistral! How are you?\"))"
      ],
      "id": "7ed58cc3",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! 😊 I'm just a virtual assistant, so I don't have feelings, but I'm here and ready to help you with anything you need! How about you—how are you doing today? Anything on your mind or something I can assist with?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "21815233",
        "outputId": "fc507236-3b0c-47b2-94ec-b847ca168207"
      },
      "source": [
        "from mistralai import Mistral"
      ],
      "id": "21815233",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'mistralai'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2856589024.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmistralai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMistral\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mistralai'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "433460b7"
      },
      "source": [
        "api_key = \"ByTX22zoItPaZeKYCOCJxdmZPj7Nc7Py\""
      ],
      "id": "433460b7",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09409d3c"
      },
      "source": [
        "def run_mistral(user_message, model=\"mistral-large-latest\"):\n",
        "    client = Mistral(api_key=api_key)\n",
        "    messages = [\n",
        "        {\"role\":\"user\", \"content\":user_message}\n",
        "    ]\n",
        "    chat_response = client.chat.complete(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "    return (chat_response.choices[0].message.content)"
      ],
      "id": "09409d3c",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "id": "6605bd48",
        "outputId": "e8a2aede-f99d-4944-d659-cadab00da313"
      },
      "source": [
        "print(run_mistral(\"Hello Mistral! How are you?\"))"
      ],
      "id": "6605bd48",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Mistral' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3423858183.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_mistral\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hello Mistral! How are you?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-892766383.py\u001b[0m in \u001b[0;36mrun_mistral\u001b[0;34m(user_message, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_mistral\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mistral-large-latest\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMistral\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     messages = [\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0muser_message\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     ]\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Mistral' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa6683dc",
        "outputId": "be1514e1-1c91-4055-b8f9-fecc967a7057"
      },
      "source": [
        "!pip install opentelemetry-api==1.37.0 opentelemetry-sdk==1.37.0"
      ],
      "id": "aa6683dc",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opentelemetry-api==1.37.0 in /usr/local/lib/python3.12/dist-packages (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-sdk==1.37.0 in /usr/local/lib/python3.12/dist-packages (1.37.0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api==1.37.0) (8.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api==1.37.0) (4.15.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk==1.37.0) (0.58b0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api==1.37.0) (3.23.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e030d9c2-1ecb-4bf0-864d-9f96b41bd016",
      "metadata": {
        "id": "e030d9c2-1ecb-4bf0-864d-9f96b41bd016"
      },
      "outputs": [],
      "source": [
        "def run_mistral(user_message, model=\"mistral-large-latest\"):\n",
        "    client = Mistral(api_key=api_key)\n",
        "    messages = [\n",
        "        {\"role\":\"user\", \"content\":user_message}\n",
        "    ]\n",
        "    chat_response = client.chat.complete(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "    return (chat_response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b98a29ef-4764-40b7-aed8-0e5d0502f985",
      "metadata": {
        "id": "b98a29ef-4764-40b7-aed8-0e5d0502f985"
      },
      "source": [
        "## Classification\n",
        "Mistral models can easily categorize text into distinct classes. In this example prompt, we can define a list of predefined categories and ask Mistral models to classify user inquiry.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d71e9d1c-ca45-4d19-882c-07e077ea19ad",
      "metadata": {
        "id": "d71e9d1c-ca45-4d19-882c-07e077ea19ad"
      },
      "outputs": [],
      "source": [
        "def user_message(inquiry):\n",
        "    user_message = (\n",
        "        f\"\"\"\n",
        "        You are a bank customer service bot. Your task is to assess customer intent\n",
        "        and categorize customer inquiry after <<<>>> into one of the following predefined categories:\n",
        "\n",
        "        card arrival\n",
        "        change pin\n",
        "        exchange rate\n",
        "        country support\n",
        "        cancel transfer\n",
        "        charge dispute\n",
        "\n",
        "        If the text doesn't fit into any of the above categories, classify it as:\n",
        "        customer service\n",
        "\n",
        "        You will only respond with the predefined category. Do not include the word \"Category\". Do not provide explanations or notes.\n",
        "\n",
        "        ####\n",
        "        Here are some examples:\n",
        "\n",
        "        Inquiry: How do I know if I will get my card, or if it is lost? I am concerned about the delivery process and would like to ensure that I will receive my card as expected. Could you please provide information about the tracking process for my card, or confirm if there are any indicators to identify if the card has been lost during delivery?\n",
        "        Category: card arrival\n",
        "        Inquiry: I am planning an international trip to Paris and would like to inquire about the current exchange rates for Euros as well as any associated fees for foreign transactions.\n",
        "        Category: exchange rate\n",
        "        Inquiry: What countries are getting support? I will be traveling and living abroad for an extended period of time, specifically in France and Germany, and would appreciate any information regarding compatibility and functionality in these regions.\n",
        "        Category: country support\n",
        "        Inquiry: Can I get help starting my computer? I am having difficulty starting my computer, and would appreciate your expertise in helping me troubleshoot the issue.\n",
        "        Category: customer service\n",
        "        ###\n",
        "\n",
        "        <<<\n",
        "        Inquiry: {inquiry}\n",
        "        >>>\n",
        "        \"\"\"\n",
        "    )\n",
        "    return user_message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f8b208e1",
      "metadata": {
        "id": "f8b208e1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "8863460a-7670-4b6f-b511-cf1df7693cfa",
      "metadata": {
        "id": "8863460a-7670-4b6f-b511-cf1df7693cfa"
      },
      "source": [
        "### Strategies we used:\n",
        "\n",
        "- **Few shot learning**: Few-shot learning or in-context learning is when we give a few examples in the prompts, and the LLM can generate corresponding output based on the example demonstrations. Few-shot learning can often improve model performance especially when the task is difficult or when we want the model to respond in a specific manner.\n",
        "- **Delimiter**: Delimiters like ### <<< >>> specify the boundary between different sections of the text. In our example, we used ### to indicate examples and <<<>>> to indicate customer inquiry.\n",
        "- **Role playing**: Providing LLM a role (e.g., \"You are a bank customer service bot.\") adds personal context to the model and often leads to better performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9d8a83cc-31e6-4d4b-b252-93d9133ecbf5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d8a83cc-31e6-4d4b-b252-93d9133ecbf5",
        "outputId": "c9f83746-a7ba-45b0-90b5-529d0d19da50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "country support\n"
          ]
        }
      ],
      "source": [
        "print(run_mistral(user_message(\n",
        "    \"I am inquiring about the availability of your cards in the EU, as I am a resident of France and am interested in using your cards. \"\n",
        ")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6eca06b-7b1d-4663-9a68-2a23116f8c6e",
      "metadata": {
        "id": "d6eca06b-7b1d-4663-9a68-2a23116f8c6e"
      },
      "outputs": [],
      "source": [
        "print(run_mistral(user_message(\"What's the weather today?\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f351f00-683a-4244-974f-5c719812141f",
      "metadata": {
        "id": "9f351f00-683a-4244-974f-5c719812141f"
      },
      "source": [
        "## Summarization\n",
        "\n",
        "Summarization is a common task for LLMs due to their natural language understanding and generation capabilities. Here is an example prompt we can use to generate interesting questions about an essay and summarize the essay.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bbab492-1d18-4832-86a9-2fba645e0e52",
      "metadata": {
        "id": "0bbab492-1d18-4832-86a9-2fba645e0e52"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "response = requests.get('https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt')\n",
        "essay = response.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaede63d-7392-4f1c-8a87-507ee31fe246",
      "metadata": {
        "id": "eaede63d-7392-4f1c-8a87-507ee31fe246"
      },
      "outputs": [],
      "source": [
        "message = f\"\"\"\n",
        "You are a commentator. Your task is to write a report on an essay.\n",
        "When presented with the essay, come up with interesting questions to ask, and answer each question.\n",
        "Afterward, combine all the information and write a report in the markdown format.\n",
        "\n",
        "# Essay:\n",
        "{essay}\n",
        "\n",
        "# Instructions:\n",
        "## Summarize:\n",
        "In clear and concise language, summarize the key points and themes presented in the essay.\n",
        "\n",
        "## Interesting Questions:\n",
        "Generate three distinct and thought-provoking questions that can be asked about the content of the essay. For each question:\n",
        "- After \"Q: \", describe the problem\n",
        "- After \"A: \", provide a detailed explanation of the problem addressed in the question.\n",
        "- Enclose the ultimate answer in <>.\n",
        "\n",
        "## Write a report\n",
        "Using the essay summary and the answers to the interesting questions, create a comprehensive report in Markdown format.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5505b0a5-411b-4804-aaef-ccecfa3d07be",
      "metadata": {
        "id": "5505b0a5-411b-4804-aaef-ccecfa3d07be",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print(run_mistral(message))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3c1535e-1156-46bc-8e28-5f71846dbe65",
      "metadata": {
        "id": "d3c1535e-1156-46bc-8e28-5f71846dbe65"
      },
      "source": [
        "## Strategies we used:\n",
        "\n",
        "- **Step-by-step instructions**: This strategy is inspired by the chain-of-thought prompting that enables LLMs to use a series of intermediate reasoning steps to tackle complex tasks. It's often easier to solve complex problems when we decompose them into simpler and small steps and it's easier for us to debug and inspect the model behavior.  In our example, we break down the task into three steps: summarize, generate interesting questions, and write a report. This helps the language to think in each step and generate a more comprehensive final report.\n",
        "- **Example generation**: We can ask LLMs to automatically guide the reasoning and understanding process by generating examples with the explanations and steps. In this example, we ask the LLM to generate three questions and provide detailed explanations for each question.\n",
        "- **Output formatting**: We can ask LLMs to output in a certain format by directly asking \"write a report in the Markdown format\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1a9cb95-bb08-4929-b16c-eb19877f3c01",
      "metadata": {
        "id": "d1a9cb95-bb08-4929-b16c-eb19877f3c01"
      },
      "source": [
        "## Personlization\n",
        "\n",
        "LLMs excel at personalization tasks as they can deliver content that aligns closely with individual users. In this example, we create personalized email responses to address customer questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cf048b4-3e33-4753-af97-25b73c51ee6a",
      "metadata": {
        "id": "9cf048b4-3e33-4753-af97-25b73c51ee6a"
      },
      "outputs": [],
      "source": [
        "email = \"\"\"\n",
        "Dear mortgage lender,\n",
        "\n",
        "What's your 30-year fixed-rate APR, how is it compared to the 15-year fixed rate?\n",
        "\n",
        "Regards,\n",
        "Anna\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36de7c1e-60c2-4f35-a51a-115b12d65bb6",
      "metadata": {
        "id": "36de7c1e-60c2-4f35-a51a-115b12d65bb6"
      },
      "outputs": [],
      "source": [
        "message = f\"\"\"\n",
        "\n",
        "You are a mortgage lender customer service bot, and your task is to create personalized email responses to address customer questions.\n",
        "Answer the customer's inquiry using the provided facts below. Ensure that your response is clear, concise, and\n",
        "directly addresses the customer's question. Address the customer in a friendly and professional manner. Sign the email with\n",
        "\"Lender Customer Support.\"\n",
        "\n",
        "\n",
        "\n",
        "# Facts\n",
        "30-year fixed-rate: interest rate 6.403%, APR 6.484%\n",
        "20-year fixed-rate: interest rate 6.329%, APR 6.429%\n",
        "15-year fixed-rate: interest rate 5.705%, APR 5.848%\n",
        "10-year fixed-rate: interest rate 5.500%, APR 5.720%\n",
        "7-year ARM: interest rate 7.011%, APR 7.660%\n",
        "5-year ARM: interest rate 6.880%, APR 7.754%\n",
        "3-year ARM: interest rate 6.125%, APR 7.204%\n",
        "30-year fixed-rate FHA: interest rate 5.527%, APR 6.316%\n",
        "30-year fixed-rate VA: interest rate 5.684%, APR 6.062%\n",
        "\n",
        "# Email\n",
        "{email}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaf50774-0f91-4e0e-86c9-1525f6045ebb",
      "metadata": {
        "id": "aaf50774-0f91-4e0e-86c9-1525f6045ebb",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print(run_mistral(message))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0af53ac9-8ef1-4840-8b23-779e1d59204d",
      "metadata": {
        "id": "0af53ac9-8ef1-4840-8b23-779e1d59204d"
      },
      "source": [
        "### Strategies we used:\n",
        "- Providing facts: Incorporating facts into prompts can be useful for developing customer support bots. It’s important to use clear and concise language when presenting these facts. This can help the LLM to provide accurate and quick responses to customer queries.\n",
        "\n",
        "## Evaluation\n",
        "\n",
        "There are many ways to evaluate LLM outputs. Here are three approaches for your reference: include a confidence score, introduce an evaluation step, or employ another LLM for evaluation.\n",
        "\n",
        "\n",
        "\n",
        "## Include a confidence score\n",
        "We can include a confidence score along with the generated output in the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bf511a7-1828-465c-92ed-a701e21b23da",
      "metadata": {
        "id": "8bf511a7-1828-465c-92ed-a701e21b23da"
      },
      "outputs": [],
      "source": [
        "def run_mistral(user_message, model=\"mistral-large-latest\"):\n",
        "    client = Mistral(api_key=api_key)\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\":\"user\",\n",
        "            \"content\": user_message\n",
        "        }\n",
        "    ]\n",
        "    chat_response = client.chat.complete(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=1,\n",
        "        response_format = {\n",
        "          \"type\": \"json_object\"\n",
        "        }\n",
        "    )\n",
        "    return (chat_response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8087cae2-9b3b-407c-b808-31fd765fd6f4",
      "metadata": {
        "id": "8087cae2-9b3b-407c-b808-31fd765fd6f4"
      },
      "outputs": [],
      "source": [
        "message = f\"\"\"\n",
        "You are a summarization system that can provide summaries with associated confidence scores.\n",
        "In clear and concise language, provide three short summaries of the following essay, along with their confidence scores.\n",
        "You will only respond with a JSON object with the key Summary and Confidence. Do not provide explanations.\n",
        "\n",
        "# Essay:\n",
        "{essay}\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c7bfcee-ff8b-4c14-b9e8-7e00d3d389ad",
      "metadata": {
        "id": "1c7bfcee-ff8b-4c14-b9e8-7e00d3d389ad"
      },
      "outputs": [],
      "source": [
        "print(run_mistral(message))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8649ad18-10c0-4fb0-92cc-052e2ab75342",
      "metadata": {
        "id": "8649ad18-10c0-4fb0-92cc-052e2ab75342"
      },
      "source": [
        "### Strategies we used:\n",
        "- JSON output: For facilitating downstream tasks, JSON format output is frequently preferred. We can enable the JSON mode by setting the response_format to `{\"type\": \"json_object\"}` and specify in the prompt that “You will only respond with a JSON object with the key Summary and Confidence.” Specifying these keys within the JSON object is beneficial for clarity and consistency.\n",
        "- Higher Temperature: In this example, we increase the temperature score to encourage the model to be more creative and output three generated summaries that are different from each other.  \n",
        "\n",
        "\n",
        "## Introduce an evaluation step\n",
        "We can also add a second step in the prompt for evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6300a485-43be-4bc4-9088-9e6c2a365b6c",
      "metadata": {
        "id": "6300a485-43be-4bc4-9088-9e6c2a365b6c"
      },
      "outputs": [],
      "source": [
        "message = f\"\"\"\n",
        "You are given an essay text and need to provide summaries and evaluate them.\n",
        "\n",
        "# Essay:\n",
        "{essay}\n",
        "\n",
        "Step 1: In this step, provide three short summaries of the given essay. Each summary should be clear, concise, and capture the key points of the speech. Aim for around 2-3 sentences for each summary.\n",
        "Step 2: Evaluate the three summaries from Step 1 and rate which one you believe is the best. Explain your choice by pointing out specific reasons such as clarity, completeness, and relevance to the speech content.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "print(run_mistral(message))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba076c25-9536-4ba1-a730-0a3ce6fe24f0",
      "metadata": {
        "id": "ba076c25-9536-4ba1-a730-0a3ce6fe24f0"
      },
      "source": [
        "## Employ another LLM for evaluation\n",
        "In production systems, it is common to employ another LLM for evaluation so that the evaluation step can be separate from the generation step.  \n",
        "\n",
        "- Step 1: use the first LLM to generate three summaries\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5265085-77fd-433a-8d9d-d89ffac76543",
      "metadata": {
        "id": "d5265085-77fd-433a-8d9d-d89ffac76543"
      },
      "outputs": [],
      "source": [
        "message = f\"\"\"\n",
        "Provide three short summaries of the given essay. Each summary should be clear, concise, and capture the key points of the essay.\n",
        "Aim for around 2-3 sentences for each summary.\n",
        "\n",
        "# essay:\n",
        "{essay}\n",
        "\n",
        "\"\"\"\n",
        "summaries = run_mistral(message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4372a9fe-d596-4f36-86c6-3e3c01c74c38",
      "metadata": {
        "id": "4372a9fe-d596-4f36-86c6-3e3c01c74c38"
      },
      "outputs": [],
      "source": [
        "print(summaries)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58fd5f25-ab9a-4547-83ab-48638517b7ef",
      "metadata": {
        "id": "58fd5f25-ab9a-4547-83ab-48638517b7ef"
      },
      "source": [
        "- Step 2: use another LLM to rate the generated summaries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b755adc8-c277-4421-8925-3d70e5ee39e7",
      "metadata": {
        "id": "b755adc8-c277-4421-8925-3d70e5ee39e7"
      },
      "outputs": [],
      "source": [
        "message = f\"\"\"\n",
        "You are given an essay and three summaries of the essay. Evaluate the three summaries and rate which one you believe is the best.\n",
        "Explain your choice by pointing out specific reasons such as clarity, completeness, and relevance to the essay content.\n",
        "\n",
        "# Essay:\n",
        "{essay}\n",
        "\n",
        "# Summaries\n",
        "{summaries}\n",
        "\n",
        "\"\"\"\n",
        "print(run_mistral(message))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e96d61a-9dbc-4f8c-9d80-d2260c5f3e61",
      "metadata": {
        "id": "3e96d61a-9dbc-4f8c-9d80-d2260c5f3e61"
      },
      "source": [
        "### Strategies we used:\n",
        "- **LLM chaining**: In this example, we chain two LLMs in a sequence, where the output from the first LLM serves as the input for the second LLM. The method of chaining LLMs can be adapted to suit your specific use cases. For instance, you might choose to employ three LLMs in a chain, where the output of two LLMs is funneled into the third LLM. While LLM chaining offers flexibility, it's important to consider that it may result in additional API calls and potentially increased costs.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "jupytext": {
      "formats": "ipynb,py:light"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}